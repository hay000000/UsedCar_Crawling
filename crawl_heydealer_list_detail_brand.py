#!/usr/bin/env python3
import csv
import time
import requests
import sys
from datetime import datetime
from pathlib import Path
from playwright.sync_api import sync_playwright

# --- ì„¤ì • ë° ê²½ë¡œ ---
# [íŠ¹ì • ê°œìˆ˜ë§Œ ìˆ˜ì§‘í•  ë•Œ] ì•„ë˜ ì£¼ì„ í•´ì œ í›„ ì‚¬ìš©
TARGET_COUNT = 10

BASE_URL = "https://www.heydealer.com"
BASE_DIR = Path(__file__).resolve().parent

# í´ë” ê²½ë¡œ ì„¤ì •
RESULT_DIR = BASE_DIR / "result" / "heydealer"
LOG_DIR = BASE_DIR / "logs" / "heydealer"
IMG_DIR = BASE_DIR / "imgs" / "heydealer"

# í´ë” ìƒì„±
RESULT_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)
IMG_DIR.mkdir(parents=True, exist_ok=True)

# íŒŒì¼ ê²½ë¡œ
LIST_FILE = RESULT_DIR / "heydealer_list.csv"
DETAIL_FILE = RESULT_DIR / "heydealer_detail.csv"

# --- ë¡œê·¸ ì„¤ì • ---
# now_date = datetime.now().strftime("%Y%m%d")
# LOG_FILE = LOG_DIR / f"heydealer_list_detail_log_{now_date}.log"
LOG_FILE = LOG_DIR / f"heydealer_list_detail.log"

class Logger(object):
    def __init__(self):
        self.terminal = sys.stdout
        self.log = open(LOG_FILE, "a", encoding="utf-8")
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
    def flush(self):
        pass

sys.stdout = Logger()

print(f"[{datetime.now()}] ğŸ í—¤ì´ë”œëŸ¬ ìˆ˜ì§‘ í”„ë¡œê·¸ë¨ ì‹œì‘")
print(f"ğŸ“ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ: {IMG_DIR}")

def load_brand_mapping():
    brand_map = {}
    brand_file = BASE_DIR / "result" / "heydealer_brands_final.csv"
    if brand_file.exists():
        with open(brand_file, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            for row in reader:
                brand_map[row['model_name'].strip()] = {
                    "brand_id": row.get('brand_id', ''),
                    "brand_name": row.get('brand_name', '')
                }
    else:
        print(f"âš ï¸ ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {brand_file}")
    return brand_map

def get_now_times():
    now = datetime.now()
    return now.strftime("%Y%m%d"), now.strftime("%Y%m%d%H%M")

def save_to_csv_append(file_path, fieldnames, data_dict):
    file_exists = Path(file_path).exists()
    with open(file_path, "a", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
        if not file_exists:
            writer.writeheader()
        writer.writerow(data_dict)

def download_image(img_url, model_cd, idx):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜"""
    try:
        if not img_url or "svg" in img_url.lower():
            return False
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Referer": BASE_URL
        }
        
        response = requests.get(img_url, stream=True, timeout=15, headers=headers)
        
        if response.status_code == 200:
            ext = img_url.split(".")[-1].split("?")[0].lower()
            if len(ext) > 4 or len(ext) < 2:
                ext = "jpg"
            
            filename = f"{model_cd}_{idx}.{ext}"
            save_path = IMG_DIR / filename
            
            with open(save_path, "wb") as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
            return True
        else:
            return False
            
    except Exception as e:
        return False

def _extract_card_heydealer(elem, idx, brand_map) -> dict:
    data = {"model_sn": idx, "brand_id": "", "brand_name": ""}
    try:
        href = elem.get_attribute("href") or ""
        full_url = (BASE_URL + href).split("?")[0] if not href.startswith("http") else href.split("?")[0]
        data["model_cd"] = full_url.split("/")[-1]
        data["detail_url"] = full_url
        m_box = elem.query_selector(".css-9j6363")
        if m_box:
            names = m_box.query_selector_all(".css-jk6asd")
            raw_model_name = names[0].inner_text().strip() if len(names) > 0 else ""
            data["model_name"] = raw_model_name
            data["model_second_name"] = names[1].inner_text().strip() if len(names) > 1 else ""
            matched = brand_map.get(raw_model_name)
            if not matched and " " in raw_model_name:
                sub_name = raw_model_name.split(" ", 1)[1].strip()
                matched = brand_map.get(sub_name)
            if matched:
                data["brand_id"], data["brand_name"] = matched["brand_id"], matched["brand_name"]
            grade = m_box.query_selector(".css-13wylk3")
            data["grade_name"] = grade.inner_text().strip() if grade else ""
        yk_el = elem.query_selector(".css-6bza35")
        if yk_el:
            txt = yk_el.inner_text().strip()
            if "ã†" in txt:
                p = txt.split("ã†")
                data["year"], data["km"] = p[0].strip(), p[1].strip()
            else: data["year"], data["km"] = txt, ""
        price_area = elem.query_selector(".css-105xtr1 .css-1066lcq .css-dbu2tk")
        if price_area:
            sale = price_area.query_selector(".css-8sjynn")
            data["sale_price"] = sale.inner_text().strip() if sale else price_area.inner_text().strip()
        d_pnttm, c_dt = get_now_times()
        data["date_crtr_pnttm"], data["create_dt"] = d_pnttm, c_dt
    except: pass
    return data

def _extract_detail_smart(page, list_item) -> dict:
    """
    ìƒì„¸ í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ + êµ¬ì¡°í™”ëœ ì´ë¯¸ì§€ ìˆ˜ì§‘
    
    êµ¬ì¡°:
    .css-1uus6sd > .css-12qft46
      â”œâ”€ ë‘ë²ˆì§¸ .css-ltrevz > .css-5pr39e > .css-1i3qy3r > .css-1dpi6xl > button.css-q47uzu > img.css-q38rgl
      â””â”€ ë„¤ë²ˆì§¸ .css-ltrevz > .css-5pr39e > .css-1i3qy3r > .css-hf19cn > .css-1a3591h > img.css-158t7i4
          â””â”€  .css-ltrevz > .css-5pr39e > .css-1i3qy3r > .css-hf19cn > .css-w9nhgi > img.css-158t7i4
    
    """
    res = {
        "model_sn": str(list_item.get("model_sn", "")),
        "brand_id": str(list_item.get("brand_id", "")),
        "brand_name": str(list_item.get("brand_name", "")),
        "model_cd": str(list_item.get("model_cd", "")),
        "model_name": str(list_item.get("model_name", "")),
        "model_second_name": str(list_item.get("model_second_name", "")),
        "grade_name": str(list_item.get("grade_name", "")),
        "year": str(list_item.get("year", "")),
        "km": str(list_item.get("km", "")),
        "refund": "", "guarantee": "", "accident": "", 
        "inner_car_wash": "", "insurance": "", "exterior_description": "", "interior_description": "", 
        "options": "", "delivery_information": "", "recommendation_comment": "",
        "tire": "", "tinting": "", "car_key": "",
        "detail_url": list_item["detail_url"],
        "date_crtr_pnttm": list_item["date_crtr_pnttm"],
        "create_dt": list_item["create_dt"]
    }
    
    try:
        try:
            page.wait_for_selector(".css-12qft46", timeout=20000)
        except Exception:
            try:
                page.wait_for_selector(".css-113wzqa", timeout=10000)
            except Exception:
                pass
        page.wait_for_timeout(2000)
        # ë ˆì´ì§€ ë¡œë”©/SPA ëŒ€ë¹„: ë¨¼ì € ìŠ¤í¬ë¡¤í•´ì„œ ì„¹ì…˜Â·ì´ë¯¸ì§€ ë¡œë“œ
        for i in range(1, 14):
            page.evaluate(f"window.scrollTo(0, {i * 500})")
            time.sleep(0.15)
        page.evaluate("window.scrollTo(0, 0)")
        page.wait_for_timeout(800)
        
        print(f"      ğŸ“¸ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹œì‘: {res['model_cd']}")
        
        # === ì´ë¯¸ì§€ ìˆ˜ì§‘ (ì €ì¥ ëŒ€ìƒ êµ¬ì¡° ì¤€ìˆ˜) ===
        # êµ¬ì¡°: .css-1uus6sd > .css-12qft46
        #   â”œâ”€ ë‘ë²ˆì§¸ .css-ltrevz > .css-5pr39e > .css-1i3qy3r > .css-1dpi6xl > button.css-q47uzu > img.css-q38rgl
        #   â””â”€ ë„¤ë²ˆì§¸ .css-ltrevz > .css-5pr39e > .css-1i3qy3r > .css-hf19cn > .css-1a3591h > img.css-158t7i4
        #       â””â”€ .css-w9nhgi > img.css-158t7i4
        downloaded_urls = set()
        img_idx = 1

        detail_container = page.query_selector(".css-1uus6sd .css-12qft46")
        if not detail_container:
            detail_container = page.query_selector(".css-12qft46")
        if detail_container:
            ltrevz_sections = detail_container.query_selector_all(".css-ltrevz")
            # print(f"      ğŸ” ë°œê²¬ëœ ì„¹ì…˜ ìˆ˜: {len(ltrevz_sections)}")

            # (1) ë‘ë²ˆì§¸ .css-ltrevz > ... > button.css-q47uzu > img.css-q38rgl
            if len(ltrevz_sections) >= 2:
                sec2 = ltrevz_sections[1]
                imgs_btn = sec2.query_selector_all(".css-5pr39e .css-1i3qy3r .css-1dpi6xl button.css-q47uzu img.css-q38rgl")
                if not imgs_btn:
                    imgs_btn = sec2.query_selector_all("button.css-q47uzu img.css-q38rgl")
                if not imgs_btn:
                    imgs_btn = sec2.query_selector_all("button img, .css-q47uzu img")
                # print(f"      ğŸ“· ìƒ‰ìƒ íŒŒíŠ¸ ì´ë¯¸ì§€: {len(imgs_btn)}ê°œ")
                for img in imgs_btn:
                    src = img.get_attribute("src") or img.get_attribute("data-src")
                    if src and src not in downloaded_urls and "svg" not in src.lower():
                        if download_image(src, res["model_cd"], img_idx):
                            downloaded_urls.add(src)
                            img_idx += 1

            # (2) ë„¤ë²ˆì§¸ .css-ltrevz > ... > .css-hf19cn > .css-1a3591h > img.css-158t7i4
            # (3) ë„¤ë²ˆì§¸ .css-ltrevz > ... > .css-hf19cn > .css-w9nhgi > img.css-158t7i4
            if len(ltrevz_sections) >= 4:
                sec4 = ltrevz_sections[3]
                for sel in [
                    ".css-5pr39e .css-1i3qy3r .css-hf19cn .css-1a3591h img.css-158t7i4",
                    ".css-5pr39e .css-1i3qy3r .css-w9nhgi img.css-158t7i4",
                    ".css-hf19cn .css-1a3591h img",
                    ".css-hf19cn .css-w9nhgi img",
                    ".css-w9nhgi img.css-158t7i4",
                ]:
                    for img in sec4.query_selector_all(sel):
                        src = img.get_attribute("src") or img.get_attribute("data-src")
                        if src and src not in downloaded_urls and "svg" not in src.lower():
                            if download_image(src, res["model_cd"], img_idx):
                                downloaded_urls.add(src)
                                img_idx += 1
                # print(f"      ğŸ“· ì´ ì´ë¯¸ì§€ ëˆ„ì : {img_idx - 1}ê°œ")

        if img_idx == 1:
            fallback_imgs = page.query_selector_all(
                "img[src*='heydealer.com'], img[src*='cdn.'], .css-w9nhgi img, .css-1a3591h img, main img"
            )
            for img in fallback_imgs:
                src = img.get_attribute("src") or img.get_attribute("data-src")
                if not src or "svg" in src.lower() or src in downloaded_urls:
                    continue
                if download_image(src, res["model_cd"], img_idx):
                    downloaded_urls.add(src)
                    img_idx += 1
            if img_idx > 1:
                print(f"      ğŸ“· í´ë°±ìœ¼ë¡œ {img_idx - 1}ê°œ ì´ë¯¸ì§€ ìˆ˜ì§‘")
        # ì„¹ì…˜ ì ê±°ë‚˜ 0ê°œì¼ ë•Œ í•œ ë²ˆ ë” ìŠ¤í¬ë¡¤ í›„ ì¬ì‹œë„ (vlgoq6l0 ë“± ì§€ì—° ë¡œë”© í˜ì´ì§€)
        if img_idx == 1:
            page.wait_for_timeout(2000)
            for i in range(1, 12):
                page.evaluate(f"window.scrollTo(0, {i * 600})")
                time.sleep(0.2)
            retry_imgs = page.query_selector_all("img[src], img[data-src]")
            for img in retry_imgs:
                src = img.get_attribute("src") or img.get_attribute("data-src")
                if not src or "svg" in src.lower() or src in downloaded_urls:
                    continue
                if "heydealer" in src or "cdn." in src or len(src) > 20:
                    if download_image(src, res["model_cd"], img_idx):
                        downloaded_urls.add(src)
                        img_idx += 1
            if img_idx > 1:
                print(f"      ğŸ“· ì¬ì‹œë„ë¡œ {img_idx - 1}ê°œ ì´ë¯¸ì§€ ìˆ˜ì§‘")
        print(f"      ğŸ“· ìƒì„¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {res['model_cd']} {img_idx - 1}ì¥")
        
        # === í˜ì´ì§€ ìŠ¤í¬ë¡¤ (ë™ì  ì½˜í…ì¸  ë¡œë”©) ===
        for i in range(1, 15):
            page.evaluate(f"window.scrollTo(0, {i * 600})")
            time.sleep(0.15)
        
        # === ìŠ¤í™ ì˜ì—­ ë¡œë“œ ëŒ€ê¸° (ë¶€ë¶„ ìˆ˜ì§‘ ë°©ì§€) ===
        for _ in range(2):
            try:
                page.wait_for_selector(".css-113wzqa", timeout=12000)
                break
            except Exception:
                page.wait_for_timeout(2000)
        page.wait_for_timeout(500)

        # === ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ ===
        option_elements = page.query_selector_all(".css-5pr39e .css-13wylk3, .css-5pr39e .css-1396o7r")
        if option_elements:
            res["options"] = ", ".join([str(opt.inner_text() or "").strip() for opt in option_elements if str(opt.inner_text() or "").strip()])

        containers = page.query_selector_all(".css-1cfq7ri")
        for container in containers:
            if "ì¶œê³  ì •ë³´" in container.inner_text():
                info_val = container.query_selector(".css-1n3oo4w")
                if info_val:
                    res["delivery_information"] = info_val.inner_text().replace("\n", " | ").strip()
                    break

        rec_el = page.query_selector(".css-yfldxx")
        if rec_el:
            res["recommendation_comment"] = rec_el.inner_text().replace("\n", " | ").strip()

        def _fill_spec_from_items(items_selector):
            filled = 0
            for item in page.query_selector_all(items_selector):
                lbl_el = item.query_selector(".css-1b7o1k1")
                if not lbl_el:
                    continue
                lbl = lbl_el.inner_text().replace(" ", "").strip()
                val_el = item.query_selector(".css-1b7o1k1 + div")
                if not val_el:
                    try:
                        raw = item.evaluate("""node => {
                            const l = node.querySelector('.css-1b7o1k1');
                            if (!l) return '';
                            const n = l.nextElementSibling;
                            return n ? (n.innerText || n.textContent || '').trim() : '';
                        }""")
                        val = str(raw).strip() if raw is not None else ""
                    except Exception:
                        val = ""
                else:
                    val = str(val_el.inner_text() or "").strip()
                if not val:
                    continue
                if "ì—°ì‹" in lbl and not res["year"]: res["year"] = val; filled += 1
                elif "ì£¼í–‰ê±°ë¦¬" in lbl and not res["km"]: res["km"] = val; filled += 1
                elif "í™˜ë¶ˆ" in lbl and not res["refund"]: res["refund"] = val; filled += 1
                elif "í—¤ì´ë”œëŸ¬ë³´ì¦" in lbl and not res["guarantee"]: res["guarantee"] = val; filled += 1
                elif "ì‚¬ê³ " in lbl and not res["accident"]: res["accident"] = val; filled += 1
                elif "ì‹¤ë‚´ì„¸ì°¨" in lbl and not res["inner_car_wash"]: res["inner_car_wash"] = val; filled += 1
                elif "ìì°¨ë³´í—˜ì²˜ë¦¬" in lbl and not res["insurance"]: res["insurance"] = val; filled += 1
                elif "ì™¸ë¶€" in lbl and not res["exterior_description"]: res["exterior_description"] = val; filled += 1
                elif "ì‹¤ë‚´" in lbl and "ì„¸ì°¨" not in lbl and not res["interior_description"]: res["interior_description"] = val; filled += 1
                elif "íƒ€ì´ì–´" in lbl and not res["tire"]: res["tire"] = val; filled += 1
                elif "í‹´íŒ…" in lbl and not res["tinting"]: res["tinting"] = val; filled += 1
                elif "ì°¨í‚¤" in lbl and not res["car_key"]: res["car_key"] = val; filled += 1
            return filled

        _fill_spec_from_items(".css-113wzqa")
        # ìŠ¤í™ì´ ë¹„ì—ˆìœ¼ë©´ ë¡œë”© ì§€ì—°ìœ¼ë¡œ ì¬ëŒ€ê¸° í›„ ì¬ì¶”ì¶œ (ìµœëŒ€ 2íšŒ)
        for _ in range(2):
            if res.get("year") or res.get("km"):
                break
            page.wait_for_timeout(3000 if _ == 0 else 5000)
            for i in range(1, 10):
                page.evaluate(f"window.scrollTo(0, {i * 400})")
                time.sleep(0.2)
            page.wait_for_timeout(1500)
            _fill_spec_from_items(".css-113wzqa")
        
        # ìˆ˜ì§‘ ê²°ê³¼
        filled_fields = sum(1 for k, v in res.items() if v and k not in ["model_sn", "model_cd", "detail_url", "date_crtr_pnttm", "create_dt"])
        total_fields = len([k for k in res.keys() if k not in ["model_sn", "model_cd", "detail_url", "date_crtr_pnttm", "create_dt"]])
        # print(f"      ğŸ“Š ë°ì´í„° í•„ë“œ: {filled_fields}/{total_fields}ê°œ ìˆ˜ì§‘")
        
    except Exception as e:
        print(f"      âŒ ìƒì„¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)[:100]}")
    
    return res

def main():
    brand_map = load_brand_mapping()
    list_fields = ["model_sn", "brand_id", "brand_name", "model_cd", "model_name", "model_second_name", "grade_name", "year", "km", "sale_price", "detail_url", "date_crtr_pnttm", "create_dt"]
    detail_fields = ["model_sn", "brand_id", "brand_name", "model_cd", "model_name", "model_second_name", "grade_name", "year", "km", "refund", "guarantee", "accident", "inner_car_wash", "insurance", "exterior_description", "interior_description", "options", "delivery_information", "recommendation_comment", "tire", "tinting", "car_key", "detail_url", "date_crtr_pnttm", "create_dt"]

    if LIST_FILE.exists(): LIST_FILE.unlink()
    if DETAIL_FILE.exists(): DETAIL_FILE.unlink()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080}
        )
        page = context.new_page()
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # [TARGET_COUNT ì‚¬ìš© ì‹œ] 
        print(f"\nğŸš€ [1ë‹¨ê³„] ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {TARGET_COUNT}ê°œ)")
        # print(f"\nğŸš€ [1ë‹¨ê³„] ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ (ëê¹Œì§€ ìŠ¤í¬ë¡¤)")
        list_url = f"{BASE_URL}/market/cars"
        for nav_try in range(3):
            try:
                page.goto(list_url, wait_until="commit", timeout=60000)
                page.wait_for_load_state("domcontentloaded", timeout=15000)
                break
            except Exception as e:
                if nav_try < 2:
                    print(f"   âš ï¸ ëª©ë¡ í˜ì´ì§€ ì¬ì‹œë„ ({nav_try + 2}/3)...")
                    time.sleep(3)
                else:
                    raise RuntimeError(f"ëª©ë¡ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {list_url}") from e
        page.wait_for_timeout(3000)

        raw_list, seen = [], set()
        prev_count = 0
        no_new_rounds = 0

        # ë¬´í•œ ìŠ¤í¬ë¡¤: ë” ì´ìƒ ìƒˆ ë§¤ë¬¼ì´ ì•ˆ ë‚˜ì˜¬ ë•Œê¹Œì§€
        while True:
            # [TARGET_COUNT ì‚¬ìš© ì‹œ] ì•„ë˜ ì£¼ì„ í•´ì œ
            if len(raw_list) >= TARGET_COUNT: # TARGET_COUNT ë‹¬ì„± ì‹œ ì¢…ë£Œ
                print(f" âœ… ëª©í‘œ ë‹¬ì„±: {TARGET_COUNT}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                break   # TARGET_COUNT ì¢…ë£Œ ì¡°ê±´ ì¶”ê°€

            last_height = page.evaluate("document.body.scrollHeight")
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(2500)

            cards = page.query_selector_all('a[href^="/market/cars/"]')
            for card in cards:
                # [TARGET_COUNT ì‚¬ìš© ì‹œ] ì•„ë˜ ì£¼ì„ í•´ì œ
                if len(raw_list) >= TARGET_COUNT: # TARGET_COUNT ë‹¬ì„± ì‹œ ì¢…ë£Œ
                    break   # TARGET_COUNT ì¢…ë£Œ ì¡°ê±´ ì¶”ê°€

                href = (card.get_attribute("href") or "").split("?")[0]
                if href and href not in seen:
                    seen.add(href)
                    item = _extract_card_heydealer(card, len(raw_list) + 1, brand_map)
                    raw_list.append(item)
                    save_to_csv_append(LIST_FILE, list_fields, item)

            # ìƒˆë¡œ ì¶”ê°€ëœ ë§¤ë¬¼ ì—†ìœ¼ë©´ ì¹´ìš´íŠ¸
            if len(raw_list) == prev_count:
                no_new_rounds += 1
            else:
                no_new_rounds = 0
            prev_count = len(raw_list)

            # [TARGET_COUNT ì‚¬ìš© ì‹œ] ì•„ë˜ë¥¼ len(raw_list)/TARGET_COUNT ëŒ€ ë¡œ ë³€ê²½
            print(f" ğŸ”„ ëª©ë¡ ìˆ˜ì§‘: {len(raw_list)}/{TARGET_COUNT}ëŒ€")   # TARGET_COUNT ì¶”ê°€
            # print(f" ğŸ”„ ëª©ë¡ ìˆ˜ì§‘: {len(raw_list)}ëŒ€")
# 

            new_height = page.evaluate("document.body.scrollHeight")
            if new_height == last_height:
                page.wait_for_timeout(2000)
                if page.evaluate("document.body.scrollHeight") == last_height:
                    print(f"ğŸ í˜ì´ì§€ ë ë„ë‹¬ (ì´ {len(raw_list)}ëŒ€)")
                    break
            else:
                no_new_rounds = 0
            # ë” ì´ìƒ ìƒˆ ë§¤ë¬¼ì´ ì•ˆ ë‚˜ì™€ë„ ì¢…ë£Œ (ìŠ¤í¬ë¡¤ì€ ë˜ì§€ë§Œ ìƒˆ ì¹´ë“œ ì—†ìŒ)
            if no_new_rounds >= 2:
                print(f"ğŸ ìƒˆ ë§¤ë¬¼ ì—†ìŒ, ìˆ˜ì§‘ ì¢…ë£Œ (ì´ {len(raw_list)}ëŒ€)")
                break

        print(f"\nğŸ“„ ëª©ë¡ CSV ìƒì„± ì™„ë£Œ: {LIST_FILE} ({len(raw_list)}ê±´)")
        print(f"\nğŸš€ [2ë‹¨ê³„] ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘ (ì´ {len(raw_list)}ëŒ€)")
        success_count = 0
        
        for idx, item in enumerate(raw_list, 1):
            success = False
            for retry in range(3):
                try:
                    retry_text = f'ì¬ì‹œë„({retry})' if retry > 0 else 'ìˆ˜ì§‘'
                    print(f"\n ğŸ” ({idx}/{len(raw_list)}) {retry_text}: {item['model_cd']}")
                    
                    page.goto(item["detail_url"], wait_until="domcontentloaded", timeout=40000)
                    page.wait_for_load_state("load", timeout=15000)
                    page.wait_for_timeout(1500)
                    detail = _extract_detail_smart(page, item)
                    # ìŠ¤í™ì´ ê±°ì˜ ë¹„ì—ˆìœ¼ë©´ í•œ ë²ˆ ë” ë¡œë“œ í›„ ì¬ì¶”ì¶œ (ë¹ˆê°’ í–‰ ê°ì†Œ)
                    spec_keys = ("year", "km", "refund", "guarantee", "accident")
                    filled_spec = sum(1 for k in spec_keys if str(detail.get(k) or "").strip())
                    if filled_spec < 2 and retry < 2:
                        page.wait_for_timeout(3000)
                        page.goto(item["detail_url"], wait_until="load", timeout=40000)
                        page.wait_for_timeout(2500)
                        detail = _extract_detail_smart(page, item)
                    # ìƒì„¸ ë¹„ì–´ ìˆìœ¼ë©´ ëª©ë¡ ê°’ìœ¼ë¡œ ì±„ì›€ (ê°’ì€ í•­ìƒ strë¡œ)
                    for k in detail_fields:
                        if k in item and not str(detail.get(k) or "").strip():
                            detail[k] = str(item.get(k) or "").strip()
                    save_to_csv_append(DETAIL_FILE, detail_fields, detail)
                    success = True
                    success_count += 1
                    break
                except Exception as e:
                    print(f"      âš ï¸ ì˜¤ë¥˜: {str(e)[:50]}")
                    if retry < 2:
                        time.sleep(2)
            
            if not success:
                print(f"      âŒ ìµœì¢… ì‹¤íŒ¨ (ëª©ë¡ ë°ì´í„°ë§Œ ì €ì¥)")
                fail_row = {k: str(item.get(k) or "") for k in detail_fields if k in item}
                for k in detail_fields:
                    if k not in fail_row:
                        fail_row[k] = ""
                save_to_csv_append(DETAIL_FILE, detail_fields, fail_row)

        print(f"\nğŸ“„ ìƒì„¸ CSV ìƒì„± ì™„ë£Œ: {DETAIL_FILE} ({success_count}ê±´)")
        print(f"\n[{datetime.now()}] âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print(f"   - ëª©ë¡: {len(raw_list)}ê°œ")
        print(f"   - ìƒì„¸ ì„±ê³µ: {success_count}/{len(raw_list)}ê°œ ({success_count/len(raw_list)*100:.1f}%)")
        print(f"   - ê²°ê³¼: {RESULT_DIR}")
        print(f"   - ì´ë¯¸ì§€: {IMG_DIR}")
        print(f"   - ë¡œê·¸: {LOG_FILE}")
        
        browser.close()

if __name__ == "__main__":
    main()